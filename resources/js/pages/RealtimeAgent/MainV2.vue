<template>
    <div
        class="bg-dot-pattern flex h-screen flex-col bg-gray-50 text-gray-900 dark:bg-gray-950 dark:text-gray-100"
        :class="{
            'screen-protection-active': isProtectionEnabled,
            'screen-protection-filter': isProtectionEnabled,
            'overlay-mode-active': isOverlayMode,
        }"
    >

        <!-- Professional Navigation Title Bar -->
        <TitleBar 
            title="Clueless"
            @dashboard-click="handleDashboardClick"
            @toggle-session="toggleSession"
        />

        <!-- Mobile Menu Dropdown -->
        <MobileMenu @dashboard-click="handleDashboardClick" />

        <!-- Main Container with scrollable layout -->
        <div class="flex flex-1 flex-col p-4 pb-6 relative min-h-0">
            <!-- Screen Protection Overlay (content area only) -->
            <div v-if="isProtectionEnabled" class="screen-protection-content-overlay" aria-hidden="true"></div>
            
            <!-- Main Content Area with Responsive Columns -->
            <!-- Mobile/Tablet: Stack all columns vertically -->
            <!-- Desktop: 3 columns side by side -->
            <div class="flex h-full flex-col gap-3 lg:grid lg:grid-cols-3">
                <!-- Column 1: Live Transcription -->
                <LiveTranscription class="min-h-[300px] lg:min-h-0" />

                <!-- Column 2: Real-time Intelligence -->
                <div class="flex flex-col gap-3">
                    <!-- Customer Intelligence Card -->
                    <CustomerIntelligence class="flex-shrink-0" />

                    <!-- Key Insights Card -->
                    <KeyInsights class="min-h-[200px] flex-1 lg:min-h-0" />

                    <!-- Post-Call Actions Card (moved from column 3) -->
                    <PostCallActions class="min-h-[150px] flex-1 lg:min-h-0" />

                    <!-- Talking Points Card -->
                    <TalkingPoints class="flex-shrink-0" />
                </div>

                <!-- Column 3: Contextual & Actions -->
                <div class="flex flex-col gap-3">
                    <!-- Contextual Information Card (50%) -->
                    <ContextualInformation
                        :prompt="selectedTemplate?.prompt || ''"
                        :conversation-context="conversationContext"
                        :last-customer-message="lastCustomerMessage"
                        class="min-h-[250px] flex-[5] lg:min-h-0"
                    />

                    <!-- Commitments Made Card (30%) -->
                    <CommitmentsList class="min-h-[150px] flex-[3] lg:min-h-0" />

                    <!-- Discussion Topics Card (20%) -->
                    <DiscussionTopics class="min-h-[120px] flex-[2] lg:min-h-0" />
                </div>
            </div>
        </div>

        <!-- Customer Info Modal -->
        <CustomerInfoModal 
            @start-with-info="startWithCustomerInfo"
            @skip="skipCustomerInfo"
        />
    </div>
</template>

<script setup lang="ts">
import { computed, onMounted, onUnmounted, watch } from 'vue';
import { router } from '@inertiajs/vue3';
import axios from 'axios';
import { 
    RealtimeAgent,
    RealtimeSession,
    type RealtimeAgentConfiguration,
} from '@openai/agents-realtime';
import { tool } from '@openai/agents-core';
import { z } from 'zod';

// Components
import TitleBar from '@/components/RealtimeAgent/Navigation/TitleBar.vue';
import MobileMenu from '@/components/RealtimeAgent/Navigation/MobileMenu.vue';
import LiveTranscription from '@/components/RealtimeAgent/Content/LiveTranscription.vue';
import CustomerIntelligence from '@/components/RealtimeAgent/Content/CustomerIntelligence.vue';
import KeyInsights from '@/components/RealtimeAgent/Content/KeyInsights.vue';
import DiscussionTopics from '@/components/RealtimeAgent/Content/DiscussionTopics.vue';
import TalkingPoints from '@/components/RealtimeAgent/Content/TalkingPoints.vue';
import CommitmentsList from '@/components/RealtimeAgent/Actions/CommitmentsList.vue';
import PostCallActions from '@/components/RealtimeAgent/Actions/PostCallActions.vue';
import CustomerInfoModal from '@/components/RealtimeAgent/Modals/CustomerInfoModal.vue';
import ContextualInformation from '@/components/ContextualInformation.vue';

// Stores
import { useRealtimeAgentStore } from '@/stores/realtimeAgent';
import { useSettingsStore } from '@/stores/settings';
import { useOpenAIStore } from '@/stores/openai';

// Composables
import { useOverlayMode } from '@/composables/useOverlayMode';
import { useScreenProtection } from '@/composables/useScreenProtection';

// Store instances
const realtimeStore = useRealtimeAgentStore();
const settingsStore = useSettingsStore();
const openaiStore = useOpenAIStore();

// Composables
const overlayMode = useOverlayMode();
const screenProtection = useScreenProtection();

// Set composable support in settings after they initialize
onMounted(() => {
    // Wait for composables to initialize
    setTimeout(() => {
        settingsStore.setOverlaySupported(overlayMode.isSupported.value);
        settingsStore.setProtectionSupported(screenProtection.isProtectionSupported.value);
        
        // Sync initial overlay mode state
        if (overlayMode.isOverlayMode.value !== settingsStore.isOverlayMode) {
            settingsStore.setOverlayMode(overlayMode.isOverlayMode.value);
        }
    }, 200);
    
    // Listen for overlay mode changes to keep store in sync
    const handleOverlayModeChange = (event: CustomEvent) => {
        if (event.detail && typeof event.detail.enabled === 'boolean') {
            settingsStore.setOverlayMode(event.detail.enabled);
        }
    };
    
    window.addEventListener('overlayModeChanged', handleOverlayModeChange as EventListener);
    
    // Cleanup on unmount
    onUnmounted(() => {
        window.removeEventListener('overlayModeChanged', handleOverlayModeChange as EventListener);
    });
});

// Computed
const selectedTemplate = computed(() => realtimeStore.selectedTemplate);
const conversationContext = computed(() => realtimeStore.conversationContext);
const lastCustomerMessage = computed(() => realtimeStore.lastCustomerMessage);
const isProtectionEnabled = computed(() => screenProtection.isProtectionEnabled.value);
// Use the overlay mode state from the composable which is the source of truth
const isOverlayMode = computed(() => overlayMode.isOverlayMode.value);

// Store SDK objects outside of Vue's reactivity to avoid proxy issues
let salespersonAgent: any = null;
let coachAgent: any = null;
let salespersonSession: any = null;
let coachSession: any = null;

// Audio capture
let audioContext: AudioContext | null = null;
let micStream: MediaStream | null = null;
let systemStream: MediaStream | null = null;
let sessionsReady = false;

// Tool definitions using SDK
const coachingTools = [
    tool({
        name: 'track_discussion_topic',
        description: 'Track or update a discussion topic',
        parameters: z.object({
            name: z.string().describe('Topic name (e.g., "Pricing", "Integration", "Timeline")'),
            sentiment: z.enum(['positive', 'negative', 'neutral', 'mixed']).describe('Sentiment of the discussion'),
            context: z.string().nullable().describe('Brief context about the discussion'),
        }),
        execute: async (args: { name: string; sentiment: string; context?: string | null }) => {
            handleFunctionCall('track_discussion_topic', args);
            return `Tracked topic: ${args.name} with ${args.sentiment} sentiment`;
        },
    }),
    
    tool({
        name: 'detect_commitment',
        description: 'Log a commitment made by either party',
        parameters: z.object({
            speaker: z.enum(['salesperson', 'customer']).describe('Who made the commitment'),
            text: z.string().describe('What was committed'),
            type: z.enum(['promise', 'next_step', 'deliverable']).describe('Type of commitment'),
            deadline: z.string().nullable().describe('When it should be done'),
            context: z.string().nullable().describe('Additional context'),
        }),
        execute: async (args: { speaker: string; text: string; type: string; deadline?: string | null; context?: string | null }) => {
            handleFunctionCall('detect_commitment', args);
            return `Captured ${args.speaker}'s commitment: ${args.text}`;
        },
    }),
    
    tool({
        name: 'analyze_customer_intent',
        description: 'Update customer intelligence analysis',
        parameters: z.object({
            intent: z.enum(['research', 'evaluation', 'decision', 'implementation', 'unknown']).describe('Customer intent'),
            buyingStage: z.string().nullable().describe('Current stage in buying process'),
            sentiment: z.enum(['positive', 'negative', 'neutral']).describe('Overall sentiment'),
            engagementLevel: z.number().min(0).max(100).describe('Engagement level percentage'),
        }),
        execute: async (args: { intent: string; buyingStage?: string | null; sentiment: string; engagementLevel: number }) => {
            handleFunctionCall('analyze_customer_intent', args);
            return `Updated customer intelligence`;
        },
    }),
    
    tool({
        name: 'highlight_insight',
        description: 'Capture a key insight from the conversation',
        parameters: z.object({
            type: z.enum(['pain_point', 'objection', 'positive_signal', 'concern', 'question']).describe('Type of insight'),
            text: z.string().describe('The insight text'),
            importance: z.enum(['high', 'medium', 'low']).describe('Importance level'),
        }),
        execute: async (args: { type: string; text: string; importance: string }) => {
            handleFunctionCall('highlight_insight', args);
            return `Captured ${args.type}: ${args.text}`;
        },
    }),
    
    tool({
        name: 'create_action_item',
        description: 'Create a post-call action item',
        parameters: z.object({
            text: z.string().describe('What needs to be done'),
            owner: z.enum(['salesperson', 'customer', 'both']).describe('Who owns this action'),
            type: z.enum(['follow_up', 'send_info', 'schedule', 'internal']).describe('Type of action'),
            deadline: z.string().nullable().describe('When it should be done'),
            relatedCommitment: z.string().nullable().describe('ID of related commitment if any'),
        }),
        execute: async (args: { text: string; owner: string; type: string; deadline?: string | null; relatedCommitment?: string | null }) => {
            handleFunctionCall('create_action_item', args);
            return `Created action item: ${args.text}`;
        },
    }),
    
    tool({
        name: 'detect_information_need',
        description: 'Detect when customer is asking about or discussing topics that require specific information',
        parameters: z.object({
            topic: z.string().describe('The main topic being discussed (e.g., "pricing", "features", "integration")'),
            context: z.string().describe('The specific context or question from the customer'),
            urgency: z.enum(['high', 'medium', 'low']).describe('How urgently the information is needed'),
        }),
        execute: async (args: { topic: string; context: string; urgency: string }) => {
            handleFunctionCall('detect_information_need', args);
            return `Detected information need about ${args.topic}`;
        },
    }),
];

// Function call handler (same as original)
const handleFunctionCall = (name: string, args: any) => {
    console.log('🔧 Handling function call:', name, args);
    console.log('Current topics:', realtimeStore.topics.length);
    console.log('Current insights:', realtimeStore.insights.length);
    
    switch (name) {
        case 'track_discussion_topic':
            realtimeStore.trackDiscussionTopic(args.name, args.sentiment, args.context);
            break;
            
        case 'analyze_customer_intent':
            realtimeStore.updateCustomerIntelligence({
                intent: args.intent,
                buyingStage: args.buyingStage || realtimeStore.customerIntelligence.buyingStage,
                sentiment: args.sentiment,
                engagementLevel: args.engagementLevel,
            });
            break;
            
        case 'highlight_insight':
            realtimeStore.addKeyInsight(args.type, args.text, args.importance);
            break;
            
        case 'detect_commitment':
            realtimeStore.captureCommitment(args.speaker, args.text, args.type, args.deadline);
            break;
            
        case 'create_action_item':
            realtimeStore.addActionItem(args.text, args.owner, args.type, args.deadline, args.relatedCommitment);
            break;
            
        case 'detect_information_need':
            console.log('📋 Information need detected:', args);
            // Could update conversation context or trigger specific responses
            realtimeStore.setConversationContext(`Customer asking about ${args.topic}: ${args.context}`);
            break;
    }
};

// Initialize function
const initialize = async () => {
    try {
        // Fetch templates
        const response = await axios.get('/templates');
        // The response has templates wrapped in a templates property
        const templates = response.data.templates || [];
        realtimeStore.setTemplates(templates);
        
        // Select first template by default
        if (templates.length > 0 && !realtimeStore.selectedTemplate) {
            realtimeStore.setSelectedTemplate(templates[0]);
        }
    } catch (error) {
        console.error('Failed to fetch templates:', error);
        // Set empty array on error to prevent filter issues
        realtimeStore.setTemplates([]);
    }
};

// Session management
const toggleSession = () => {
    if (realtimeStore.isActive) {
        endCall();
    } else {
        realtimeStore.setShowCustomerModal(true);
    }
};

const startWithCustomerInfo = (info: { name: string; company: string }) => {
    realtimeStore.setCustomerInfo(info);
    startCall();
};

const skipCustomerInfo = () => {
    startCall();
};

const startCall = async () => {
    try {
        realtimeStore.setConnectionStatus('connecting');
        
        // Get ephemeral key
        const { data } = await axios.post('/api/realtime/ephemeral-key');
        
        if (data.status === 'error') {
            throw new Error(data.message || 'Failed to get ephemeral key');
        }
        
        openaiStore.setEphemeralKey(data.ephemeralKey, 60);
        
        // Initialize agents
        await initializeAgents(data.ephemeralKey);
        
        // Start audio capture
        console.log('🎤 Starting audio capture...');
        await startAudioCapture();
        
        realtimeStore.setActiveState(true);
        realtimeStore.setConnectionStatus('connected');
        
        // Auto-enable screen protection during calls
        screenProtection.enableForCall();
        
        // Test audio connection
        setTimeout(() => {
            console.log('🔍 Audio capture status check:', {
                isActive: realtimeStore.isActive,
                microphoneStatus: realtimeStore.microphoneStatus,
                systemAudioActive: realtimeStore.systemAudioActive,
                audioLevel: realtimeStore.audioLevel,
                systemAudioLevel: realtimeStore.systemAudioLevel,
            });
        }, 3000);
        
        // Add initial system message
        realtimeStore.addTranscriptGroup({
            id: `system-${Date.now()}`,
            role: 'system',
            messages: [{ text: 'Call started. Coach is ready to assist.', timestamp: Date.now() }],
            startTime: Date.now(),
        });
        
    } catch (error) {
        console.error('Failed to start call:', error);
        realtimeStore.setConnectionStatus('disconnected');
    }
};

const endCall = async () => {
    try {
        // Stop microphone stream
        if (micStream) {
            micStream.getTracks().forEach(track => track.stop());
            micStream = null;
        }
        
        // Stop system audio stream
        if (systemStream) {
            systemStream.getTracks().forEach(track => track.stop());
            systemStream = null;
        }
        
        // Disable audio loopback if enabled
        if ((window as any).audioLoopback) {
            try {
                await (window as any).audioLoopback.disableLoopback();
                console.log('Audio loopback disabled');
            } catch (e) {
                console.log('Failed to disable audio loopback:', e);
            }
        }
        
        // Close audio context
        if (audioContext) {
            await audioContext.close();
            audioContext = null;
        }
        
        // Disconnect sessions directly to avoid Vue proxy issues
        if (salespersonSession) {
            try {
                salespersonSession.close();
            } catch (error) {
                console.error('Error closing salesperson session:', error);
            }
            salespersonSession = null;
        }
        
        if (coachSession) {
            try {
                coachSession.close();
            } catch (error) {
                console.error('Error closing coach session:', error);
            }
            coachSession = null;
        }
        
        // Clear agents
        salespersonAgent = null;
        coachAgent = null;
        
        // Reset sessions ready flag
        sessionsReady = false;
        
        // Clear from store
        openaiStore.clearAgents();
        
        realtimeStore.setActiveState(false);
        realtimeStore.setConnectionStatus('disconnected');
        
        // Add end message
        realtimeStore.addTranscriptGroup({
            id: `system-${Date.now()}`,
            role: 'system',
            messages: [{ text: 'Call ended.', timestamp: Date.now() }],
            startTime: Date.now(),
        });
        
    } catch (error) {
        console.error('Failed to end call:', error);
    }
};

const initializeAgents = async (apiKey: string) => {
    // Create salesperson agent (transcription only)
    const salespersonConfig: RealtimeAgentConfiguration = {
        name: 'Salesperson',
        description: 'Transcribes the salesperson audio',
        instructions: 'You are a transcription assistant. Your only job is to transcribe what the salesperson says.',
        tools: [],
    };
    
    salespersonAgent = new RealtimeAgent(salespersonConfig);
    salespersonSession = new RealtimeSession(salespersonAgent, {
        transport: 'websocket',
        model: 'gpt-4o-mini-realtime-preview-2024-12-17',
        apiKey: apiKey,
    });
    
    // Create coach agent with tools
    const coachConfig: RealtimeAgentConfiguration = {
        name: 'Customer Coach',
        description: 'Analyzes conversations and provides real-time coaching',
        instructions: selectedTemplate.value?.prompt || 'You are a helpful sales coach.',
        tools: coachingTools,
    };
    
    coachAgent = new RealtimeAgent(coachConfig);
    coachSession = new RealtimeSession(coachAgent, {
        transport: 'websocket',
        model: 'gpt-4o-mini-realtime-preview-2024-12-17',
        apiKey: apiKey,
    });
    
    // Set up event handlers
    setupSessionHandlers();
    
    // Connect sessions
    await salespersonSession.connect({});
    await coachSession.connect({});
    
    // Configure sessions after connection
    console.log('🔧 Configuring sessions for audio...');
    
    // Configure salesperson session for transcription
    if (salespersonSession?.transport) {
        salespersonSession.transport.sendEvent({
            type: 'session.update',
            session: {
                modalities: ['audio', 'text'],
                instructions: 'You are a transcription assistant. Your only job is to transcribe what you hear.',
                input_audio_format: 'pcm16',
                output_audio_format: 'pcm16',
                input_audio_transcription: {
                    model: 'whisper-1',
                },
                turn_detection: {
                    type: 'server_vad',
                    threshold: 0.5,
                    prefix_padding_ms: 300,
                    silence_duration_ms: 200,
                },
                voice: 'alloy',
                temperature: 0.8,
                max_response_output_tokens: 4096,
            },
        });
        console.log('✅ Salesperson session configured for audio transcription');
    }
    
    // Configure coach session with tools
    if (coachSession?.transport) {
        const basePrompt = selectedTemplate.value?.prompt || 'You are a helpful sales coach.';
        const coachInstructions = `${basePrompt}

CRITICAL: You must actively and continuously analyze EVERY piece of conversation in real-time:

1. IMMEDIATELY track discussion topics as they are mentioned using track_discussion_topic
2. INSTANTLY detect any commitments using detect_commitment - even tentative ones
3. CONTINUOUSLY highlight insights using highlight_insight for any important points
4. ALWAYS analyze customer intent with analyze_customer_intent after each customer statement
5. PROACTIVELY detect information needs using detect_information_need

Be EXTREMELY aggressive in calling these functions - it's better to over-analyze than miss something.
Call multiple functions per statement if relevant. Don't wait for complete thoughts - analyze partial information.`;
        
        // Convert tools to the format expected by OpenAI
        const toolDefinitions = coachingTools.map(tool => ({
            type: 'function',
            name: tool.name,
            description: tool.description,
            parameters: tool.parameters,
        }));
        
        coachSession.transport.sendEvent({
            type: 'session.update',
            session: {
                modalities: ['audio', 'text'],
                instructions: coachInstructions,
                input_audio_format: 'pcm16',
                output_audio_format: 'pcm16',
                input_audio_transcription: {
                    model: 'whisper-1',
                },
                turn_detection: {
                    type: 'server_vad',
                    threshold: 0.3,              // More sensitive to speech
                    prefix_padding_ms: 200,      // Less padding before speech
                    silence_duration_ms: 100,    // Shorter silence to trigger processing
                },
                voice: 'alloy',
                temperature: 0.8,
                max_response_output_tokens: 4096,
                tools: toolDefinitions,
                tool_choice: 'auto',
            },
        });
        console.log('✅ Coach session configured with tools:', toolDefinitions.length);
    }
    
    // Mark sessions as ready
    sessionsReady = true;
    console.log('✅ All sessions configured and ready for audio');
    
    // Store in OpenAI store
    openaiStore.setSalespersonAgent(salespersonAgent, salespersonSession);
    openaiStore.setCoachAgent(coachAgent, coachSession);
};

const setupSessionHandlers = () => {
    if (!salespersonSession || !coachSession) return;
    
    // Debug logging to understand session structure
    console.log('Setting up session handlers...');
    console.log('Salesperson session:', salespersonSession);
    console.log('Coach session:', coachSession);
    
    // Listen for all events on transport layer for debugging
    if (salespersonSession.transport) {
        salespersonSession.transport.on('*', (event: any) => {
            if (event.type !== 'input_audio_buffer.append') { // Don't log audio append events
                console.log('Salesperson transport event:', event.type, event);
            }
        });
    }
    
    if (coachSession.transport) {
        coachSession.transport.on('*', (event: any) => {
            if (event.type !== 'input_audio_buffer.append') { // Don't log audio append events
                console.log('Coach transport event:', event.type, event);
            }
        });
    }
    
    // Salesperson session handlers
    salespersonSession.on('conversation.updated', (event: any) => {
        console.log('Salesperson conversation updated:', event);
        // Note: Transcript handling moved to conversation.item.input_audio_transcription.completed
    });
    
    // Coach session handlers
    coachSession.on('conversation.updated', (event: any) => {
        console.log('Coach conversation updated:', event);
        // Note: Transcript handling moved to conversation.item.input_audio_transcription.completed
    });
    
    // Function call handlers for coach analytics
    coachSession.on('response.function_call_arguments.done', (event: any) => {
        console.log('✅ Coach function call complete:', event);
        if (event.name && event.arguments) {
            try {
                const args = JSON.parse(event.arguments);
                handleFunctionCall(event.name, args);
            } catch (e) {
                console.error('❌ Failed to parse function arguments:', e);
            }
        }
    });
    
    coachSession.on('response.done', () => {
        console.log('✅ Coach response complete');
    });
    
    // Handle transcription events - these are the primary events for capturing audio transcripts
    if (salespersonSession.transport) {
        salespersonSession.transport.on('conversation.item.input_audio_transcription.completed', (event: any) => {
            console.log('Salesperson transcription completed:', event);
            if (event.transcript) {
                // Try to append to last group if same speaker
                const appended = realtimeStore.appendToLastTranscriptGroup('salesperson', event.transcript);
                
                if (!appended) {
                    // Create new group if not appended
                    const groupId = `salesperson-${Date.now()}`;
                    realtimeStore.addTranscriptGroup({
                        id: groupId,
                        role: 'salesperson',
                        messages: [{ text: event.transcript, timestamp: Date.now() }],
                        startTime: Date.now(),
                    });
                }
                
                // Forward to coach for analysis
                if (coachSession && coachSession.transport) {
                    coachSession.transport.sendEvent({
                        type: 'conversation.item.create',
                        item: {
                            type: 'message',
                            role: 'user',
                            content: [{
                                type: 'input_text',
                                text: `Salesperson said: "${event.transcript}"`
                            }]
                        }
                    });
                }
            }
        });
    }
    
    if (coachSession.transport) {
        coachSession.transport.on('conversation.item.input_audio_transcription.completed', (event: any) => {
            console.log('Customer transcription completed:', event);
            if (event.transcript) {
                // Try to append to last group if same speaker
                const appended = realtimeStore.appendToLastTranscriptGroup('customer', event.transcript);
                
                if (!appended) {
                    // Create new group if not appended
                    const groupId = `customer-${Date.now()}`;
                    realtimeStore.addTranscriptGroup({
                        id: groupId,
                        role: 'customer',
                        messages: [{ text: event.transcript, timestamp: Date.now() }],
                        startTime: Date.now(),
                    });
                }
                
                // Update last customer message and context
                realtimeStore.setLastCustomerMessage(event.transcript);
                
                // Update conversation context with recent messages
                const recentTranscripts = realtimeStore.transcriptGroups
                    .slice(-5)
                    .map(g => `${g.role}: ${g.messages.map(m => m.text).join(' ')}`)
                    .join('\n');
                realtimeStore.setConversationContext(recentTranscripts);
            }
        });
    }
    
    
    // Error handlers
    salespersonSession.on('error', (error: any) => {
        console.error('Salesperson session error:', error);
        realtimeStore.addTranscriptGroup({
            id: `error-${Date.now()}`,
            role: 'system',
            messages: [{ text: `Salesperson connection error: ${error.message || JSON.stringify(error)}`, timestamp: Date.now() }],
            startTime: Date.now(),
            systemCategory: 'error',
        });
    });
    
    coachSession.on('error', (error: any) => {
        console.error('Coach session error:', error);
        realtimeStore.addTranscriptGroup({
            id: `error-${Date.now()}`,
            role: 'system',
            messages: [{ text: `Coach connection error: ${error.message || JSON.stringify(error)}`, timestamp: Date.now() }],
            startTime: Date.now(),
            systemCategory: 'error',
        });
    });
};

const checkMicrophonePermission = async () => {
    try {
        // Check if we're in Electron environment on macOS
        if (window.location.protocol === 'nativephp:' && navigator.platform.includes('Mac')) {
            // Check current microphone permission status
            const statusResponse = await axios.get('/api/system/media-access-status/microphone');
            console.log('🎤 Microphone permission status:', statusResponse.data.status);
            
            if (statusResponse.data.status !== 'granted') {
                // Request microphone permission
                console.log('🎤 Requesting microphone permission...');
                const permissionResponse = await axios.post('/api/system/ask-for-media-access', {
                    mediaType: 'microphone'
                });
                
                if (!permissionResponse.data.granted) {
                    throw new Error('Microphone permission denied');
                }
                console.log('✅ Microphone permission granted');
            }
        }
    } catch (error) {
        console.error('Error checking microphone permission:', error);
        // Continue anyway - the browser will handle permissions
    }
};

const startAudioCapture = async () => {
    try {
        console.log('🎤 Setting up dual audio capture...');
        
        // Check microphone permission first on macOS
        await checkMicrophonePermission();
        
        // Setup microphone capture for salesperson's voice
        micStream = await navigator.mediaDevices.getUserMedia({
            audio: {
                sampleRate: 24000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
            },
        });
        realtimeStore.setMicrophoneStatus('active');
        console.log('✅ Microphone access granted (Salesperson audio)');
        
        // Create audio context with 24kHz sample rate (required by OpenAI)
        audioContext = new AudioContext({ sampleRate: 24000 });
        
        // Resume audio context if it's suspended
        if (audioContext.state === 'suspended') {
            await audioContext.resume();
            console.log('✅ Audio context resumed');
        }
        
        // Setup microphone processor
        const micSource = audioContext.createMediaStreamSource(micStream);
        const micProcessor = audioContext.createScriptProcessor(4096, 1, 1);
        
        micProcessor.onaudioprocess = (event) => {
            if (realtimeStore.isActive && sessionsReady) {
                const inputData = event.inputBuffer.getChannelData(0);
                
                // Update audio level for visualization
                const sum = inputData.reduce((acc, val) => acc + Math.abs(val), 0);
                realtimeStore.setAudioLevel(Math.min(100, (sum / inputData.length) * 500));
                
                // Convert to PCM16 and send audio
                const pcm16 = convertFloat32ToPCM16(inputData);
                
                // Send to salesperson session
                if (salespersonSession && salespersonSession.transport) {
                    const base64Audio = arrayBufferToBase64(pcm16.buffer);
                    try {
                        salespersonSession.transport.sendEvent({
                            type: 'input_audio_buffer.append',
                            audio: base64Audio,
                        });
                        
                        // Log occasionally for debugging
                        if (Math.random() < 0.02) {
                            console.log('🎤 Sent audio to salesperson session', {
                                audioLength: base64Audio.length,
                                samples: pcm16.length,
                                sessionsReady,
                                transportConnected: !!salespersonSession.transport,
                            });
                        }
                    } catch (error) {
                        console.error('Failed to send audio to salesperson:', error);
                    }
                }
            } else if (!sessionsReady && Math.random() < 0.1) {
                console.log('⏳ Audio ready but sessions not configured yet');
            }
        };
        
        micSource.connect(micProcessor);
        micProcessor.connect(audioContext.destination);
        
        // Try to setup system audio capture using electron-audio-loopback
        try {
            console.log('🔊 Setting up system audio capture with electron-audio-loopback...');
            
            // Check if audio loopback is available
            if (!(window as any).audioLoopback) {
                throw new Error('Audio loopback not available');
            }
            
            // Enable audio loopback
            try {
                await (window as any).audioLoopback.enableLoopback();
                console.log('✅ Audio loopback enabled');
            } catch (e) {
                console.error('Failed to enable audio loopback:', e);
                throw e;
            }
            
            // Get system audio stream using getDisplayMedia
            try {
                const displayStream = await navigator.mediaDevices.getDisplayMedia({
                    audio: true,
                    video: true
                });
                
                // Remove video tracks, keep only audio
                const videoTracks = displayStream.getTracks().filter(t => t.kind === 'video');
                videoTracks.forEach(t => {
                    t.stop();
                    displayStream.removeTrack(t);
                });
                
                systemStream = displayStream;
                console.log('✅ System audio stream obtained');
                
                // Create audio processor for system audio
                const systemSource = audioContext.createMediaStreamSource(systemStream);
                const systemProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                
                systemProcessor.onaudioprocess = (event) => {
                    if (realtimeStore.isActive && sessionsReady) {
                        const inputData = event.inputBuffer.getChannelData(0);
                        
                        // Update system audio level for visualization
                        const sum = inputData.reduce((acc, val) => acc + Math.abs(val), 0);
                        realtimeStore.setSystemAudioLevel(Math.min(100, (sum / inputData.length) * 500));
                        
                        // Convert to PCM16 and send audio
                        const pcm16 = convertFloat32ToPCM16(inputData);
                        
                        // Send to coach session
                        if (coachSession && coachSession.transport) {
                            const base64Audio = arrayBufferToBase64(pcm16.buffer);
                            try {
                                coachSession.transport.sendEvent({
                                    type: 'input_audio_buffer.append',
                                    audio: base64Audio,
                                });
                                
                                // Log occasionally for debugging
                                if (Math.random() < 0.05) {
                                    console.log('📞 Sent system audio to coach session', {
                                        audioLength: base64Audio.length,
                                        samples: pcm16.length,
                                        level: realtimeStore.systemAudioLevel,
                                        sessionsReady,
                                        transportConnected: !!coachSession.transport,
                                    });
                                }
                            } catch (error) {
                                console.error('Failed to send audio to coach:', error);
                            }
                        }
                    }
                };
                
                systemSource.connect(systemProcessor);
                systemProcessor.connect(audioContext.destination);
                
                // Set system audio as active
                realtimeStore.setSystemAudioActive(true);
                
                // Handle stream end
                systemStream.getTracks().forEach(track => {
                    track.addEventListener('ended', () => {
                        console.log('System audio track ended');
                        realtimeStore.setSystemAudioActive(false);
                    });
                });
                
                realtimeStore.addTranscriptGroup({
                    id: `system-${Date.now()}`,
                    role: 'system',
                    messages: [{ text: '✅ Dual audio capture active: Microphone (You) + System Audio (Customer)', timestamp: Date.now() }],
                    startTime: Date.now(),
                    systemCategory: 'success',
                });
                
            } catch (error) {
                // Disable loopback if getDisplayMedia failed
                try {
                    await (window as any).audioLoopback.disableLoopback();
                } catch (e) {
                    console.error('Failed to disable loopback after error:', e);
                }
                throw error;
            }
            
        } catch (error) {
            console.warn('⚠️ System audio capture failed:', error);
            realtimeStore.setSystemAudioActive(false);
            
            // Check if user cancelled screen share
            if (error instanceof Error && error.name === 'NotAllowedError') {
                realtimeStore.addTranscriptGroup({
                    id: `system-${Date.now()}`,
                    role: 'system',
                    messages: [
                        { text: '⚠️ Screen share cancelled. System audio not captured.', timestamp: Date.now() },
                        { text: '• Using microphone only mode', timestamp: Date.now() },
                        { text: '• To enable system audio, restart the call and share your screen', timestamp: Date.now() },
                    ],
                    startTime: Date.now(),
                    systemCategory: 'warning',
                });
            } else {
                // Other errors - fallback to mixed audio mode
                realtimeStore.addTranscriptGroup({
                    id: `system-${Date.now()}`,
                    role: 'system',
                    messages: [
                        { text: '⚠️ System audio capture unavailable. Using mixed audio mode.', timestamp: Date.now() },
                        { text: '• Use headphones to reduce echo', timestamp: Date.now() },
                        { text: '• AI will identify speakers based on conversation context', timestamp: Date.now() },
                    ],
                    startTime: Date.now(),
                    systemCategory: 'warning',
                });
            }
        }
        
        console.log('✅ Audio pipeline setup complete');
        
    } catch (error) {
        console.error('❌ Failed to setup audio:', error);
        realtimeStore.setMicrophoneStatus('error');
        throw error;
    }
};

// Helper function to convert Float32Array to Int16Array
const convertFloat32ToPCM16 = (float32Array: Float32Array): Int16Array => {
    const pcm16 = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
        const clamped = Math.max(-1, Math.min(1, float32Array[i]));
        pcm16[i] = Math.floor(clamped * 32767);
    }
    return pcm16;
};

// Helper function to convert ArrayBuffer to base64
const arrayBufferToBase64 = (buffer: ArrayBuffer): string => {
    const uint8Array = new Uint8Array(buffer);
    let base64Audio = '';
    const chunkSize = 8192; // Process in 8KB chunks
    
    for (let i = 0; i < uint8Array.length; i += chunkSize) {
        const chunk = uint8Array.slice(i, Math.min(i + chunkSize, uint8Array.length));
        base64Audio += btoa(String.fromCharCode(...chunk));
    }
    
    return base64Audio;
};


const handleDashboardClick = () => {
    if (!realtimeStore.isActive) {
        router.visit('/conversations');
    }
};

// Watch for template changes
watch(selectedTemplate, (newTemplate) => {
    if (newTemplate && coachSession?.transport) {
        // Update coach instructions via transport
        coachSession.transport.sendEvent({
            type: 'session.update',
            session: {
                instructions: newTemplate.prompt,
            },
        });
    }
});

// Developer console methods
const enableMockMode = () => {
    console.log('🎭 Enabling mock mode with pre-loaded data...');
    realtimeStore.enableMockMode();
    console.log('✅ Mock mode enabled with conversation history, insights, and action items');
    return 'Mock mode enabled';
};

const disableMockMode = () => {
    console.log('🎭 Disabling mock mode...');
    realtimeStore.disableMockMode();
    console.log('✅ Mock mode disabled');
    return 'Mock mode disabled';
};

// Expose to window for console access
if (typeof window !== 'undefined') {
    (window as any).clueless = {
        enableMockMode,
        disableMockMode,
    };
}

// Lifecycle
onMounted(() => {
    initialize();
    
    // Close dropdowns on outside click
    document.addEventListener('click', () => {
        settingsStore.closeAllDropdowns();
    });
    
    // Log developer console commands
    console.log('🛠️ Developer Commands Available:');
    console.log('  window.clueless.enableMockMode() - Load mock conversation data');
    console.log('  window.clueless.disableMockMode() - Disable mock mode');
});

onUnmounted(async () => {
    if (realtimeStore.isActive) {
        await endCall();
    }
    
    // Clean up
    openaiStore.reset();
    document.removeEventListener('click', settingsStore.closeAllDropdowns);
});
</script>

<style>
/* Apple-style Glassmorphism */
.glass-card {
    background: rgba(255, 255, 255, 0.7);
    backdrop-filter: blur(20px);
    -webkit-backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.3);
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.15);
}

.glass-card-darker {
    background: rgba(255, 255, 255, 0.5);
    backdrop-filter: blur(20px);
    -webkit-backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.2);
}

/* Clean, minimal transitions */
.glass-card {
    transition: all 0.3s ease;
}

.glass-card:hover {
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.25);
    transform: translateY(-1px);
}

/* Dark mode glass cards */
:is(.dark *) .glass-card {
    background: rgba(0, 0, 0, 0.4);
    backdrop-filter: blur(20px);
    -webkit-backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.1);
    box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.3);
}

:is(.dark *) .glass-card-darker {
    background: rgba(0, 0, 0, 0.6);
    backdrop-filter: blur(20px);
    -webkit-backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.05);
    box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.5);
}

/* Custom scrollbar styles for better visibility */
.custom-scrollbar::-webkit-scrollbar {
    width: 6px;
}

.custom-scrollbar::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.1);
    border-radius: 3px;
}

.custom-scrollbar::-webkit-scrollbar-thumb {
    background: rgba(139, 92, 246, 0.3);
    border-radius: 3px;
}

.custom-scrollbar::-webkit-scrollbar-thumb:hover {
    background: rgba(139, 92, 246, 0.5);
}

/* Dark mode scrollbar */
:is(.dark *) .custom-scrollbar::-webkit-scrollbar-track {
    background: rgba(255, 255, 255, 0.1);
}

:is(.dark *) .custom-scrollbar::-webkit-scrollbar-thumb {
    background: rgba(139, 92, 246, 0.5);
}

:is(.dark *) .custom-scrollbar::-webkit-scrollbar-thumb:hover {
    background: rgba(139, 92, 246, 0.7);
}

/* Subtle animations */
@keyframes fadeIn {
    from {
        opacity: 0;
        transform: translateY(-5px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

.animate-fadeIn {
    animation: fadeIn 0.2s ease-out;
}
</style>